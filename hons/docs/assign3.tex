\documentstyle{article}

\author{Nic Hollingum - 308193415}
\title{Research Methods - Assignment 3}

\begin{document}

\maketitle

\section*{Literature Review}

This research area focuses on a combination of several others.
Foremost it deals with The Synchronous Dataflow paradigm, this model, as well as several implementations and competing models are analysed.
The major issue facing multiprocessor/computer paradigms is scheduling, which is known to be NP-Hard.
The scheduling heuristics and approximations proposed in the iterature are examined for their applicability and quality.
Finally We look at the requirements and implementations of systems calling themselves 'fault tolerant', and we compare the effectiveness of such systems.

\subsection*{SDF and Parallel Paradigms}

The Synchronous Dataflow (SDF) paradigm is just one of many programming paradigms.
It builds on the more general computation graphs described in \cite{kar66}, 44 years ago.
At the time that model was only a graph-theoretical representation of parallel computation, and was more of an abstraction rather than a paradigm.
We compare this with the more fleshed-out description of the fundamentals of the language in \cite{sdfBook}.
This work is an amalgamation of previous input on the SDF issue, and is considered the defacto description of the paradigm.
We are particularly interested in the calculation of actor repetitions described first by \cite{lee87} and appearing in \cite{sdfBook}.
This is important because it is the static calculation of demand and supply by actors (called blocks by Lee) that allows SDF to be optimized statically, giving it advantages over dynamic systems such as S-Net described in \cite{pen09}.

Dynamic dataflow systems like S-Net, is deemed necessary for solving certain classes of problems pure SDF cannot.
This is deemed to be one of the major problems with StreaMIT, as stated by Thies in \cite{thies02}.
SDF requires static declaration fo actor consumptions and productions on each of its channels, which allows the calculation of Lee's repetitions vector.
However This added difficulty was presumed to be difficult by Thies and even corroborated by programmers in \cite{thies10}.
That said systems which allow the same level of abstractions without static IO rates are difficult to optimize for multiprocessor environments.
Implementations of S-Net described in \cite{pen09} usually involve around a Master-Worker paradigm, which is not sufficiently fault tolerant for our purposes.

S-Net and Streamit are not the only dataflow implementations, a much earlier work can be found in \cite{cas87}.
In this work LUSTRE is presented, which is declarative as opposed to functional (StreaMIT).
The language presented seems difficult to work with, and implements several novel notions of computation (sequence operators, clocking) that normal declarative languages do not deal with, which is understandable given the stream paradigm.
However The notion of data-dependant clocks is an important one.
We contrast this to the distributed clocking system presneted by Lamport in \cite{lam78}.
Both Lamport and Caspi et al. strike the distinction between physical clocks and logical clocks, so as to be able to deal with the notion of precedence in distributed systems.
This is a necessary distinction, since it doesnt actually matter the physical time of an execution, but only the time relative to the execution of other distributed components.
Lamport defines a ``happened before'' relationship among processes which imposes a partial ordering of the system at the level of processes, whereas in LUSTRE data precedence is defined by an incremental counter (called the clock) assigned in a tuple with each data item.
Both process and data clocking is important in SDF implementations, processes must not execute before they have data to execute on, and data must not be consumed or produced out-of-order.

Given a partial ordering of data items and processes on distributed systems in general and SDF-like systems specifically, we are now able to explot the parallelism inherant in SDF.
A simplistic means of doing this is to assign actors to processors, in a similar fashion to \cite{par03}.
The distributed process networks described in that paper have the ability to scale well with instance size, given sufficiently good load balancing.
However that paper only deals with worker count, in the hope of data independance and stateless processes.
The paper therefore deals more with buffer management and load balancing, in order to achieve ``Embarrasing parallelism''.
Embarassingly parallel programs are programs with no data interdependencies (i.e. unordered) and no process states (in our case, statless actors).
This presumption is not allowed in general for SDF programs, whose actors can have arbitary state and whose tokens are necessarily ordered.
This is not a problem for the most well known massively-parallel paradigm, MapReduce.
MapReduce is quite a common industry tool these days for large scale data-crunching, as described in \cite{dea08}.
Much of the work goes into distributed-memory grid/cloud systems, however implementations such as PHOENIX \cite{ran07} demonstrate the paradigm's viability for shared-memory multithread systems.

We shall examine these systems with regard to efficiency and fault-tolerance  in later sections of the review.

\subsection*{Scheduling}

Scheduling, especially parallel scheduling, is in the NP-Hard class of problems \cite{len87, kha94}, most intuitively shown by a reduction to multi-knapsack by Litke et al. in \cite{lit07}.
We must therefore consider heuristic and approximation approaches to the problem.
More specifically, our problem involves a 3-way optimization of the scheduling goal: makespan, communication cost and fault tolerance.
Since fault tolerance is more of a constraint than an optimization goal, we shall ignore it for now.

Optimizing for makespan alone is the simpler of the two remaining issues.
We use te procedure outlined by Lenstra and Tardos in \cite{len87}.
This paper describes the linear relaxation of an integer programme which computes an exact solution to the makespan problem.
Whilst the integrality constraint makes the problem NP-Hard, the relaxation is $O(n^2)$ and so computing an actor-processor assignment mapping can be done in a reasonable ammount of time.
The Heuristic bound for the procedure they describe is 2, with $3/2$ being the absolute heuristic bound, times as bad as the optimal assignment in the worst case.
This bound is shown by representing the mapping problem as a bipartite graph.

Whilst this assignment is acceptable for most applications it makes an unreasonable assumption for us and imposes an unreasonable requirement.
Namely it presumes that processors can communicate with each other instantaneously, and that we are allowd to do as much inter-processor communication as we like.
However our problem permits neither of these.
The algorithm described by Boykov et al. in \cite{boy01}, whilst apparently unrelated (image segmentation), allows us to schedule to minimize communication cost.
This formulation is an ILP once again, however the proof of the approximation bound (again 2 times the optimal) is shown as a multiway cut problem.



Orchestrating Streamit \cite{mal08}

Scheduling Heuristic comparisaon \cite{Kha94}

Functional partitioning \cite{li10}

Energy minimization \cite{boy01}

hetrogeneous scheduling \cite{len87}

OLP \cite{hen99}

cloud dataflow \cite{tsa09}

\subsection*{Fault Tolerance}

Byzantine \cite{lam86}

software fault-tolerance \cite{ran75}

reliability challenges \cite{ree06}

mobile grid fault tolerance\cite{lit07}



We examine the history and implementations of SDF and similar parallel paradigms so as to determine the strengths and weaknesses of these systems.
We are mainly concerned with fault tolerance.
The early implementations such as LUSTRE \cite{cas87} do not even mention fault-tolerance, possibly due to a dirth of distributed systems at the time.
Indeed the notion of fault tolerance was relatively new at the time \cite{ran75}.
For the modern implementations we look at S-Net, StreaMIT and MapReduce and their descriptionf of ``fault tolerance''.
In later sections of the literature review we shall specify this concept more formally.
\begin{itemize}
	\item S-Net provides little by way of fault tolerance \cite{pen09}.  
	\item{StreaMIT} ascskdbvksdn.
	\item{MapReduce}dk dkcnsdk dkjcskdbck.
\end{itemize}

\section*{Research Proposal}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
