\chapter{Background}
\label{chapBack}

\section{Synchronous Dataflow}

\subsection{Streaming Systems}
The entire domain of streaming systems is diverse and loosely defined.
For convenience streaming programs can be though of as those which deal with ``flows'' of data rather than the more traditional Von Neumann computational model.
To that end streaming systems can range from dataflow systems \cite{gur85} to signal processing \cite{sdfBook}.

\begin{figure}
\begin{center}
	\input{figures/backDataflow}
\caption{A simple computational graph.  This one computes $ax^2 + bx + c$}
\label{figSimpleDG}
\end{center}
\end{figure}

Stream programs can be though of as computational graphs, often visualised as directed graphs, such as the one in Figure \ref{figSimpleDG}.
In future we shall say that such a graph has $n$ nodes and $m$ arcs.
In general systems the graphs have some nodes which provide inputs for the system, other nodes which transform data, and finally nodes which publish this data.
In such graphs data is not a discrete finite unit but rather a (possibly infinite) stream, where each node in the graph performs an atomic computation, possibly consuming a subsequence of its inputs and possibly producing more data on its outputs.
In this way the data streams are said to flow along the edges of the graph, these edges are therefore called {\em channels}.

As a rule there are no restrictions applied to such computational graphs.
However as we shall see there may be some limitations we can apply to these graphs in order to gain desirable properties.
These restrictions are detailed in section \ref{BACK_SYNC}.

Stephens has compiled a more complete survey \cite{ste97} on the range of implementations for the varying flavours of stream systems.
In that work he makes special note of the lack of coherence or similarity between differing implementations or even purposes for stream computation.
To this end {\em streaming computation} is not a distinct theory for the computer sciences.

\subsection{Synchronous Streaming}
\label{BACK_SYNC}

Synchronous streaming systems as the name would suggest are a subset of all streaming systems.
Specifically streaming systems where the execution (called {\em invocation}) of the nodes in the computational graph is related to or conrolled by the execution of others.
This necessarily implies some kind of global or local logical discrete clock.

The reason synchronous systems are noteworthy is that they have some useful properties relating to the size of streams, the execution of nodes and the allocation of memory buffers.
We shall look more closely at the synchronous streaming paradigm {\em synchronous dataflow} (SDF) to demonstrate this.

\begin{figure}
\begin{center}
	\input{figures/backSimpleSDF}
\caption{Some SDF programs represented as graphs, these graphs are reproduced from \cite{sdfBook}}
\label{figSimpleSDF}
\end{center}
\end{figure}

In SDF the computational units (called {\em actors}) have limitations imposed on their execution by augmenting the channels with a ``production to consumption'' ratio.
Figure \ref{figSimpleSDF} shows some sample SDF graphs.
Suppose a channel streams data from actor $a$ to $b$, where one invocation of $a$ produces $i$ data items (called {\em tokens}) and one invocation of $b$ consumes $j$ tokens.
Then we would annotate the SDF graph by writing $i$ near the $a$ end of the channel, and $j$ near the $b$ end.
More formally this relation is between $producer$ and $consumer$ actos along a channel.
If we called that channel $e$ then $prod(e) = a$, $cons(e) = b$, $productions(e, prod(e)) = i$ and $consumptions(e, cons(e)) = j$.

At any given point in time there are some number of tokens in buffers in the system waiting to be consumed by actors.
Tokens which exist on an actor's input channels before invocation are called $delays$, note that given the actor must have sufficient delays on its channel (that is, $>= consumptions(c, cons(c))$) before it can be invoked.
The gloabl configuration of delays on all channels is called the {\em fill state}.
The first desirable property of SDF is how it maintains the fill state.

Early work by Lee and Messerschmitt details the calculation of a comodity called the {\em repetitions vector} \cite{lee87}.
To calculate this we first express the SDF graph as a {\em topological matrix}, which is an $n \times m$ matrix $\Gamma$ where $\Gamma_{i,j}$ is $productions(i, j)$ if actor i produces to channel j, $-consumptions(i, j)$ if actor i consumes from channel j, and zero otherwise.
Importantly if the rank of this matrix is $n-1$ then the repetitions vector can be computed.
Figure \ref{figSimpleTop} shows the topological matrix $\Gamma$ and repetitions vector $r$ for graph b) in figure \ref{figSimpleSDF}.
For a description of the properties of the repetitions vector and its computation see \cite{sdfBook}.
Importantly if we invoke each actor exactly the number of times the repetitions vector details, then the fill state of the graph remains unchanged.
This solves two key problems associated with unrestricted streaming systems.
Firstly each actor must be invoked at least once, which prevents {\em starvation} of certain channles, leading to deadlocks.
Secondly each actor is executed a finite number of times, which allows the memory buffers to have bounded size.
When there are precisely enough tokens in every channel for every actor to execute the number of times required by the repetitions vector, then the channels are said to be {\em saturated}.
Executing actors according to this vector is called the {\em steady state} schedule.

\begin{figure}
\begin{center}
\[
	\Gamma = \begin{bmatrix}
	2 & -1 & 0 \\
	0 & 1 & -1 \\
	2 & 0 & -1 \\
	2 & 0 & -1
	\end{bmatrix}
	r = \begin{bmatrix}
		1 \\
		2 \\
		2
	\end{bmatrix}
\]
\caption{The Topological Matrix and Repetitions vector for the graph in Figure \ref{figSimpleSDF} b.}
\label{figSimpleTop}
\end{center}
\end{figure}

\subsection{Applications}

Applications of streaming computations can be as varied as its implementations.
The original use of streaming systems was for analysing continuous flows of data, such as markets.
Around the same time work was being done on continuous machines (called dataflow machines) such as the manchester prototype machine \cite{gur85}.
These illustrate the two areas of interest for streaming systems; software and hardware, notably both of which operate on streams of items (data and instructions respectively).
However the data need not be conceptualised as streams in order for streaming programs to be useful.

More modern applications leverage the inherant parallelism in computational graphs to speed up common applications such as databases and data-mining.
A more complete description of these systems can be found in the Related Works section.
Streaming solutions lend themselves naturally to the kind of large scale and often distributed databases that companies deal with regularly.

The other large area of use is in signal processing and other variants of real time systems.
Environmental and status monitors, as well as astronomical software, right through to media applications like video processing (and video playing \cite{thies02}) fall under this category.
Applications of this kind benefit from the more natural expression of the streaming domain, as well as benefiting from parallelism.

\section{Fault Tolerance}
\subsection{Motivating Problems}
Fault tolerance (FT) has a broad meaning in regards to computer systems.
Intuitively we think of FT as some mechanisms or practices inherent in systems that prevent them from failing completely under adverse circumstances.
Though this understanding lacks formal grounding, as we shall see it is useful to begin with.
Most computer users will experience faults on a regular basis, ranging from bugs or errors to more serious security flaws and instability.
Systems which are able to ignore or recover from such problems may be called fault tolerant.

Whilst home systems may benefit from some ammount of FT, this becomes an absolure requirement for large systems.
To demonstrate this let us treat faults like a random variable, and say that every hour a computer has a small chance of serious fault (CPU error, disk corruption, terminal bugs, power failure etc.) causing the computer to go offline.
Reed et al observed that even given a reasonably small probability, such that an individual computer fails on average after $10^6$ hours, in large systems composed of 10000 computers, at least one component is likely to fail in hunder 100 hours \cite{ree06}.
Large systems therefore require stronger guarantees for FT.

However not all FT mechanisms are the same.
We shall detail the actual mechanisms in the following sections, but to compare them we must state what the desirable properties of FT are.
\begin{itemize}
	\item	The system must be able to recover from arbitrary faults.
			Lamport and Smith's early work on {\em byzantine} fault tolerance \cite{lam86} informs our understading here.
			The intuition is that the systems designer cannot know in advance which nodes are likely to fail or how, and therefore must cope with the worst possible kinds of faults, namely malicious attacks.
			That work crucially deals with the notion that a fault can come from any source.
			Systems such as Google's MapReduce \cite{dea08} are able to handle most faults, however are unable to recover from faults in certain key machines.
	\item	The overhead for FT must be reasonable.
			More intuitively it shouldnt cost more to prevent certain kinds of faults than the fault itself costs.
			Clearly this requirement will vary between situations.
			It is one of the research goals of this work to examine the tradeoffs necessary for various FT mechanisms, one of which is overhead.
\end{itemize}

\subsection{Replication}
{\em Replication} is a generally applicable way to ensure FT both in computer systems and the real world.
This mechanism may be alternatively called {\em duplication} or {\em redundancy} with slightly varied flavours of meaning, however the principal is the same.

This mechanism involves providing multiple instances ({\em replicates}) of the same task.
So long as any one of the replicates succeeds then the computation will succeed, in other words with $r$ replicates, if any one replicate fails with probability $p$ then the entire process fails with probability $p^r$, given naive assumptions about failure independance.

This mechanism is particularly useful for large systems and systems with relatively heavyweight components.
In the case of large systems, the replication is usualy viable given sufficient processors over which to duplicate, and these redundant elements significantly increase the mean time to failure.
However users are more farmiliar with the concept of redundant heavyweight components, particularly hard disks in RAID.
RAID as the name suggests is a replication strategy providing fault tolerance or performance gains (or both), depending on the configuration.

In this work we examine the FT and overhead of duplicating SDF actors accross multiple processors.
This overhead is compared with another FT mechanism, checkpointing.

\subsection{Checkpointing}
The {\em checkpointing} mechanism is an alternative to the replication method described above.
Though many systems do indeed checkpoint, for our work all systems which recover from faults by storing information about current or past computations will be called ``checkpointing'' systems.

In this mechanism, some information about the system's state is stored in a reliable way, so that it can be recovered in the event of a crash.
One way is to write the node's state to disk before performing a computation, in this way if a node fails during that computation the previous state can be recovered.
However the recovery information need not be kept or tracked by individual nodes, in the case of Google's MapReduce the state is stored by the master controller, and in the case of a worker fault the master redistributes the task to a different worker \cite{dea08}.

This kind of FT is well researched in the area of databases \cite{dbrec}.
Modern databases use various checkpointing methodologies (called {\em commiting}) to make guarantees about the system's robustness.
These mechanisms are interesting, because they must work on standalone machines, and are not specifically designed for parallel problems.
Because of this individual computers are able to provide guarantees for robustness, and therefore can be orchestrated to provide robust parallel systems.