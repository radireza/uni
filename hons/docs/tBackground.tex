\chapter{Background}
\section{Synchronous Dataflow}

\subsection{Streaming Systems}
The entire domain of streaming systems is diverse and loosely defined.
For convenience streaming programs can be though of as those which deal with ``flows'' of data rather than the more traditional Von Neumann computational model.
To that end streaming systems can range from {\em dataflow} type systems to {\em signal processing}.

%CHEESE
{\em Here we have a picture of a typical D-Graph}

Stream programs can be though of as computational graphs, often visualised as directed graphs.
In future we shall say that such a graph has $n$ nodes and $m$ arcs.
In general systems the graphs have some nodes which provide inputs for the system, other nodes which transform data, and finally nodes which publish this data.
In such graphs data is not a discrete finite unit but rather a (possibly infinite) stream, where each node in the graph performs an atomic computation, possibly consuming a subsequence of its inputs and possibly producing more data on its outputs.
In this way the data streams are said to flow along the edges of the graph, these edges are therefore called {\em channels}.

As a rule there are no restrictions applied to such computational graphs.
However as we shall see there may be some limitations we can apply to these graphs in order to gain desirable properties.
These restrictions are detailed in \ref{BACK_SYNC}.

Stephens has compiled a more complete survey \cite{ste97} on the range of implementations for the varying flavours of stream systems.
In that work he makes special note of the lack of coherence or similarity between differing implementations or even purposes for stream computation.
To this end {\em streaming computation} is not a distinct theory for the computer sciences.

\subsection{Synchronous Streaming}
\label{BACK_SYNC}

Synchronous streamins systems as the name would suggest are a subset of all streaming systems.
Specifically streaming systems where the execution (called {\em invocation}) of the nodes in the computational graph is related to or conrolled by the execution of others.
This necessarily implies some kind of global or local logical discrete clock.

The reason synchronous systems are noteworthy is that they have some useful properties relating to the size of streams, the execution of nodes and the allocation of memory buffers.
We shall look more closely at the synchronous streaming paradigm {\em synchronous dataflow} (SDF) to demonstrate this.

%CHEESE
{\em Here we have a picture of a typical SDF program}

%CHEESE
{\em i may change around the description below when i have a diagram to discuss/explain}

In SDF the computational units (called {\em actors}) have limitations imposed on their execution by augmenting the channels with a ``production to consumption'' ratio.
Suppose a channel streams data from actor $a$ to $b$, where one invocation of $a$ produces $i$ data items (called {\em tokens}) and one invocation of $b$ consumes $j$ tokens.
Then we would annotate the SDF graph by writing $i$ near the $a$ end of the channel, and $j$ near the $b$ end.
More formally this relation is between $producer$ and $consumer$ actos along a channel.
If we called that channel $e$ then $prod(e) = a$, $cons(e) = b$, $productions(e, prod(e)) = i$ and $consumptions(e, cons(e)) = j$.

At any given point in time there are some number of tokens in buffers in the system waiting to be consumed by actors.
Tokens which exist on an actor's input channels before invocation are called $delays$, note that given the actor must have sufficient delays on its channel (that is, $>= consumptions(c, cons(c))$) before it can be invoked.
The gloabl configuration of delays on all channels is called the {\em fill state}.
The first desirable property of SDF is how it maintains the fill state.

Early work by Lee and Messerschmitt details the calculation of a comodity called the {\em repetitions vector} \cite{lee87}.
To calculate this we first express the SDF graph as a {\em topological matrix}, which is an $n \times m$ matrix $\Gamma$ where $\Gamma_{i,j}$ is $productions(i, j)$ if actor i produces to channel j, $-consumptions(i, j)$ if actor i consumes from channel j, and zero otherwise.
Importantly if the rank of this matrix is $n-1$ then the repetitions vector can be computed.
For a description of the properties of the repetitions vector and its computation see \cite{sdfBook}.
Importantly if we invoke each actor exactly the number of times the repetitions vector details, then the fill state of the graph remains unchanged.
This solves two key problems associated with unrestricted streaming systems.
Firstly each actor must be invoked at least once, which prevents {\em starvation} of certain channles, leading to deadlocks.
Secondly each actor is executed a finite number of times, which allows the memory buffers to have bounded size.
Executing actors according to this vector is called the {\em steady state} schedule.

%CHEESE
{\em Here we put the topological matrix and repetitions vector of the above SDF program}

\subsection{Applications}

Applications of streaming computations can be as varied as its implementations.
The original use of streaming systems was for analysing continuous flows of data, such as markets.
Around the same time work was being done on continuous machines (called dataflow machines) such as the manchester prototype machine \cite{gur85}.
These illustrate the two areas of interest for streaming systems; software and hardware, notably both of which operate on streams of items (data and instructions respectively).
However the data need not be conceptualised as streams in order for streaming programs to be useful.

More modern applications leverage the inherant parallelism in computational graphs to speed up common applications such as databases and data-mining.
A more complete description of these systems can be found in the Related Works section.
Streaming solutions lend themselves naturally to the kind of large scale and often distributed databases that companies deal with regularly.

The other large area of use is in signal processing and other variants of real time systems.
Environmental and status monitors, as well as astronomical software, right through to media applications like video processing (and video playing \cite{thies02}) fall under this category.
Applications of this kind benefit from the more natural expression of the streaming domain, as well as benefiting from parallelism.

\section{Cloud Computers}
Cloud computers are a general term for service-based computing systems.
In recent years advancements in network technologies have made cloud systems popular, and providing software to run on these systems can be challenging.
Although there are many nuances of ``cloud systems'' we are primarily concerned with those made up of hundreds to thoosands of stand-alone computers.

SDF applications are particularly important to cloud systems as they provide easy decomposition of tasks.

\section{Fault Tolerance}
Fault tolerance is becomming a more important requirement for systems as they increase in size.
Current implementations of SDF systems \cite{mal08, thies02, thies10} do not explicitly guarantee fault tolerance in execution.
This is an acceptable shortfall in the case of standalone systems, where a ``node failure'' means the whole computer dies, and so no mechanisms can exist to recover from such a failure, the computation must be rerun.
However in the cloud/grid context mean time to failure (MTTF) can be as short as 100h \cite{ree06}.
As such, mechanisms must be put in place to ensure successful computation given a few node failures.

Fault tolerance can be achieved through several mechanisms:
\begin{itemize}
	\item Replication is where multiple versions of the same computation take place on different machines.  By mechanisms described by Lamport and Smith in \cite{lam86}, so long as no less than two thirds of the computations succeed, the failing computations can be ignored.
			These systems are also called ``redundant''.  Some variations of RAID are typical examples of fault tolerance using this mechanism.
	\item Checkpointing is where successful states are saved to disk, so that in the event of a failure, the last successful state can be reverted to and computation re-performed.
\end{itemize}

Systems with these kinds of fault-tolerance have been implemented in the past \cite{ree06, lit07} with some success.
All these systems involve expending some kind of overhead or wasting some computational power on redundant executions.
This is generally the case for fault-tolerant systems, however the ammount of time/storage/computation wasted on overheads are not equal and the extent to which successful execution can be ensured varies.
For example, the checkpointing method will never fail completely, in the sense that correct state can always be recovered and computation re-run, whereas the methods involving some redundancy may fail completely in the (rare) case that all elements fail.
On the other hand, checkpointing requires enough space to store successful states, whereas replication does not increase space overhead, but requires more computation both to duplicate tasks and to decide on successful computations.
One of the aims of this research is to find if (and when) the costs of such overheads can decide the appropriate fault tolerant mechanisms.
