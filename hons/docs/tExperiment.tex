\chapter{Experimental Results}
\label{chapExperiment}

\section{Tolerance Experiments}

\subsection{Correct Functioning of Mechanisms}

First we must show the correct recovery of the system after the occurrence of a fault.
These are the most basic kinds of experiments and are very controlled.
Our simulator uses mostly randomly-generated graphs as problem instances, and we devise a simple deterministic invocation behaviour which allows us to determine if the fault-tolerance is working correctly.

Our simple program works as follows:
\begin{itemize}
	\item If an actor does not have any input channels it remembers an internal state which begins at 0.
			This internal state used to represent some input.
			These actors write an increasing series of natural numbers to their ourput channels as objects of type double.
			So an actor with no input channels and two output channels each producing two tokens per invocation, will write $(0,1)$ on the first channel, then $(2,3)$ on the second, on its first invocation, then $(4, 5)$ and $(6, 7)$ on its second.
	\item Actors with some inputs and some outputs consume all tokens and average them to determine their state.
			As above, the actor then writes these as double objects to each of their output channels in increasing fashion.
			So an actor with two input channels consuming one token each and one output channel producing three, may read $(3)$ and $(4.5)$ and will produce $(3.75, 4.75, 5.75)$.
	\item Actors with no outputs print the average of all input tokens to standard output.
\end{itemize}
\noindent This arbitrary choice of functionality allows us to observe deterministic outputs from arbitrary graphs.
Note that the output will vary with graph topology and repetition counts.
For this experiment we use simple acyclic graphs such as the ones shown in Figure \ref{figRGraph}.

\begin{figure}
\begin{center}
	\input{figures/exExamples}
\caption{Examples of the kind of graphs used for random tests}
\label{figRGraph}
\end{center}
\end{figure}

We run the simulator as described in Chapter \ref{chapSystems}.
For this test we choose a graph with four nodes (depicted as graph $a$ in Figure \ref{figRGraph}) to be run on five simulated machines over three steady state cycles.
The output of this program is shown here:

\begin{center}
\begin{tabular}{ | p{5cm} p{5cm} | }
	\hline
	\begin{verbatim}
	-- Steady State 0 --
	{2} - 5.083333333333333
	{2} - 11.25
	{2} - 17.416666666666668
	-- Steady State 1 --
	{2} - 26.083333333333336
	{2} - 32.25
	{2} - 38.41666666666667
	\end{verbatim}
	&
	\begin{verbatim}
	-- Steady State 2 --
	{2} - 47.08333333333333
	{2} - 53.25
	{2} - 59.41666666666667
	\end{verbatim}
	\\ \hline
\end{tabular}
\end{center}

\noindent In each steady state the actor called \verb={2}= is invoked three times, and each time averages the inputs of its tokens to print a result to standard output.

Now we run the simulator on the same graph where each actor is replicated twice (left) and three times (right) to produce the following:
\input{figures/exOutput}
\noindent Note that the actor formerly called \verb={2}= has been replicates by the actors \verb={4}= and \verb={5}= in the first case, and \verb={6}=, \verb={7}= and \verb={8}= in the second.
Since these actors are replicas, the simulator appends their names with an index numbering their order amongst replicas.
We can see that the duplicated output is identical to the output for single executions.
The actors execute in parallel using Java Threads, so the order that machines print to standard out varies.

Now that we have our expected output, we will introduce a fault in one of the machines during the second steady state.
First when the actors are duplicated, using replication FT:
\begin{center}
\begin{tabular}{ | p{5cm} p{5cm} | }
	\hline
	\begin{verbatim}
-- Steady State 0 --
{4}[0] - 5.083333333333333
{4}[0] - 11.25
{4}[0] - 17.416666666666668
{5}[1] - 5.083333333333333
{5}[1] - 11.25
{5}[1] - 17.416666666666668
-- Steady State 1 --
{5}[1] - 26.083333333333336
{5}[1] - 32.25
{5}[1] - 38.41666666666667
## 4> Farewell cruel world!
	\end{verbatim}
	&
	\begin{verbatim}
-- Steady State 2 --
processor 4 is dead
## Fault Detected
{5}[1] - 47.08333333333333
{5}[1] - 53.25
{5}[1] - 59.41666666666667
## Fault Detected
## Fault Detected
	\end{verbatim}
	\\ \hline
\end{tabular}
\end{center}
\noindent By coincidence the actor called \verb={4}= also resides on processor four, which prints "Farewell cruel world!" when it faults.
Since the processor where \verb={4}= resides has faulted, it does not print output, however the other processor executes its actors, which inform the server that they detect faults while trying to communicate with that processor.
For this test we simply print the detection message to standard output.
Note the output is still correct.

Next we use recomputation-based fault-tolerance mechanism on the original (un-replicated) example:
\begin{center}
\begin{tabular}{ | p{5cm} p{5cm} | }
	\hline
	\begin{verbatim}
-- Steady State 0 --
{2} - 5.083333333333333
{2} - 11.25
{2} - 17.416666666666668
-- Steady State 1 --
{2} - 26.083333333333336
{2} - 32.25
## 0> Farewell cruel world!
== Begin checkpoint recovery ==
restarting machine 0
initialise machine 0
machine 0 reloading state 0
machine 1 reloading state 0
machine 2 reloading state 0
machine 3 reloading state 0
machine 4 reloading state 0
Recovery complete
	\end{verbatim}
	&
	\begin{verbatim}
-- Steady State 1 --
{2} - 26.083333333333336
{2} - 32.25
{2} - 38.41666666666667
-- Steady State 2 --
{2} - 47.08333333333333
{2} - 53.25
{2} - 59.41666666666667
	\end{verbatim}
	\\ \hline
\end{tabular}
\end{center}
\noindent Under recomputation FT the fault detection immediately begins the recovery process.
The faulted machine is restarted and the fill-state for each processor is reloaded from disk for all machines.
Since the last successful steady state was 0, this state is reloaded and recomputation begins again at state 1, even though state 1 was partially completed before the machine faulted.

\subsection{Multiple Failure Experiments}
\label{secExMultiFail}

We run our SDF programs now under multiple-fault simulations.
Again these tests are designed to demonstrate the robustness of the FT mechanisms, as such we will for brevity only point out that the simulation goes as expected.
Since these tests involve failing processors at random, they are run multiple times and the results discussed are for the average case.

The replication FT mechanism is known to fail when all replicas of any one actor are on machines which fail.
In experiments we observe that for few replicas, the probability of failure differs from what is expected.
The estimates presented in Chapter \ref{chapModel} make the assumption that actors are assigned to processors uniformly at random.
Whilst this simplifies the model, in practice this is seldom true.
In order to minimise communication costs, processors are typically assigned all actors, or in other words, only as many processors as duplicates are used.
So long as all the actors are assigned to fewer processors, this affects the probability of failure.

Let us suppose that we must assign sufficiently many actors ($n$) replicated sufficiently many times($r$) to a small number of processors($p$).
When we randomly assign, we choose for each group of replicas, $r$ out of $p$ processors to assign them to.
This can be done in $\binom{p}{r}$ different ways.
If any one set of $r$ processors dies, this will fault the system, however there are $n$ randomly chosen sets of processors, any one of which will cause a fault.
The chances of randomly faulting enough processors to cause this is quite high.
In contrast the heuristic and optimal solvers tend to assign all the actors to only $r$ of the processors, meaning there is only one combination of processors which must break in order to fault the system.
The chances of randomly faulting those specific processors is much lower.

The recomputation FT mechanism is guaranteed to terminate correctly for anything less than 100\% failure rate.
However under tests with multiple failures a weakness was exposed for recurring failures.
Again this weakness is related to how spread-out the actors are on the processors.
When a fault is encountered the simulator must rollback to the last successful execution, however occasionally failures would occur again before another steady state had completed.
With sufficiently high probability of failure and sufficiently spread out processors, the simulator rarely completes an entire steady-state execution.
In contrast, when the actors were grouped to fewer processors (especially to only one processor) the performance improved significantly, due to the relative improbability of that important processor failing.
Experimentally, executing five steady states on five machines with probability failure of 0.1 per machine per steady state, required on average 3.2 resets when the actors were spread out, and 0.9 resets when the actors were not.

\section{Heuristic and Optimal Assignment Costs}

\subsection{No-Duplication Assignments}

For these experiments we formulate random problem instances with no duplicates, and use an ILP solver, running the model formulated in Chapter \ref{chapModel}, and our Heuristic solver (Chapter \ref{chapAlgos}) to perform the assignment.
The purpose of these experiments is two-fold:
\begin{itemize}
	\item {\bf To examine the performance of the heuristic on more general graphs}.
			These tests are run on randomly generated graphs, whereas the duplication process produces graphs of a distinct kind.
			Figure \ref{figDupCompare} shows how the general structure of duplicated graphs compares to the original.
			The graphs produced are more connected, and the requirement that certain nodes be assigned to different processors forces communication, which, as we observed in Section \ref{secExMultiFail}, the heuristic and optimal solvers typically avoid in the un-duplicated case.
	\item {\bf To compare the results for ILP solvers against the Heuristic solver on stream graphs with a small number of processors}.
			In practice, the processing requirements for instances larger than 15 nodes exceed reasonable running times (roughly 24 hrs on a Quad-Core i7 1.6GHz).
			Our reason for this is that when we duplicate nodes we do so uniformly, so even graphs with as few as five nodes can reach the limit of processing time with only three-way duplication.
			Tests on duplicated instances will be presented in the next section but these have typically fewer actors than the no-duplication instances shown here.
\end{itemize}


\begin{figure}
\begin{center}
	\input{figures/exCompare}
\caption{Diagram shows a random graph and its duplicated instance}
\label{figDupCompare}
\end{center}
\end{figure}

We generate multiple instances with varying actor count and connectedness, though we keep the number of processors constant.
Table \ref{figSTable} shows the assignment costs of the heuristic in comparison with the optimal solution.
The columns of this table show:
\begin{itemize}
	\item {\bf n}: The number of actors in the problem.
	\item {\bf c}: The number of instances tested where the random generation produced a graph with $n$ nodes.
	\item {\bf Div}: The number of times the heuristic did not match the optimal assignment.
	\item {\bf AO}: The average cost of optimal assignments.
	\item {\bf AH}: The average cost of heuristic assignments.
	\item {\bf AP}: The average proportion between heuristic and optimal.
	\item {\bf DO}: The average cost of optimal assignments where the heuristic assignment was different.
	\item {\bf DH}: The average cost of heuristic assignments where the optimal assignment was different.
	\item {\bf DP}: The average proportion between heuristic and optimal where the two assignments did not match.
\end{itemize}

\begin{table}
\begin{center}
	\input{figures/exSingle}
\caption{Performance of heuristic assignment on no-duplicate problems}
\label{figSTable}
\end{center}
\end{table}

This experiment shows us an interesting property of the heuristic, it seems to perform more poorly on larger instances.
In the no-duplication case the communication component between actors can be completely ignored by assigning all actors to the same processor.
Since this significantly reduces the assignment cost, the heuristic and optimal will typically choose this.
However whereas the optimal is choosing the best processor for all actors, the heuristic is choosing the best processor for the first actor it finds, then assigning all other actors to that processor to avoid communication cost.
The heuristic can be thought of as choosing an actor at random and assigning all actors to the processor that that actor happens to run best on.
Unsurprisingly then, the heuristic is unlikely to choose this actor when there are more actors to choose from.
Thus we see when we look at the Div column in Table \ref{figSTable} that when we only have 5-6 actors, we have suboptimal heuristic assignments 16\%-25\% of the time, but when we have 9-10 actors, we have 37\%-50\% suboptimal heuristic assignments.

This experiment also shows an interesting property in the degree of suboptimality.
When we look at only suboptimal assignments, although the likelyhood of a suboptimal assignment increases, we see that the optimal to heuristic ratio does not necessarily increase with problem size.
For these experiment communication costs are both random and semi-random (certain computers are on average better), so we would expect little proportional cost increase with problem size.
The results bear out this conjecture.

These experiments are run on problem instances where invocation cost is modelled either totally at random, or where certain machines invoke actors generally at lower cost.
The experiments in Table \ref{figSTable} have an equal number of both kinds of experiment.
It is possible to devise pathological cases where the heuristic performs arbitrarily worse than the optimal, however such instances do not capture realistic problems.

\subsection{Multiple Replica Assignments}
\label{secExMulAssign}

This section analyses how the optimal solver and heuristic perform when their problem's actor-set is replicated.
We analyse the same problem under various k-way duplications with the same processor description (and therefore invocation times), however, the duplicated instances will require more communication cost in general.

\begin{table}
\begin{center}
	\input{figures/exFrequency}
\caption{Number of tests run on duplicate assignments}
\label{figExFreq}
\end{center}
\end{table}

These experiments by necessity constituted larger problem sizes.
As such, certain experiments were not run due to time limitations.
Optimal assignment formulations which exceed 10 hours in run time were excluded from these tests.
The number of tests which could be completed in the time limit given the un-duplicated actor size (row) and k-way duplication (column) are shown in Table \ref{figExFreq}.
Note that the heuristic solver takes under a second for all tests, most of which is file IO.

\begin{table}
\begin{center}
	\input{figures/exRampup}
\caption{Average optimal assignment cost}
\label{figExRamp}
\end{center}
\end{table}

First we look at the cost of optimal assignment over various k-way duplications.
Table \ref{figExRamp} shows the average of all optimal assignment costs for the given $n$-actor $k$-duplicate cases.
As we would expect, when we look down the columns for the 1 and 2-way duplications, we see linearly increasing costs.
This is the intuitive result since we have a linearly increasing problem sizes.
However when we look across the rows we see much faster cost increases.
According to the model discussed in Chapter \ref{chapModel}, we would expect to see polynomially increasing costs.
Unfortunately only the $n = 3$ and $n = 4$ tests have enough samples to show such an increase, and in those cases we see such an increase.

\begin{table}
\begin{center}
	\input{figures/exAccuracy}
\caption{Average performance (heuristic cost/optimal cost) of heuristic assignment}
\label{figExAcc}
\end{center}
\end{table}

Now we see if the heuristic's performance degrades as the number of replicas increases.
Table \ref{figExAcc} shows the proportional increase in cost of the heuristic against the optimal solution.
On examination we see few patterns in the heuristic performance as the number of replicas increases.
That is to say that while we see increases in the $n=4, 6$ cases, where $n$ is the number of actors, we see decreases in the $n=3, 5$ cases.
We also see no uniform increase or decrease in all the k-way replication of the $n=3$ case.
The lack of a pattern here is surprising, given the results of the experiment in Table \ref{figSTable}.
In that experiment we saw that the heuristic's performance worsened as the problem size increased, however when we replicate (and therefore increase the problem size) we do not lose out on the heuristic.

An explanation for this finding is that the heuristic has less of an ability to err when there are more replicas.
In the un-replicated case, the assignment was essentially to one processor, and so as the problem size increased the chance of assigning to that one correct processor was less.
However, in the replicated case there are $k$ correct processors, and the heuristic is likely to randomly choose at least some of the correct ones.
Hence the number of correct processors scales with the number of replicas, which accounts for the lack of a performance drop-off with problem size in the heuristic.

\section{Assignment Properties}

In this section we use the experiments to confirm some of the more basic properties of the assignment problem.
These observations are noteworthy, though it is quite clear why the problem has those traits.

Processor utilisation is an issue of some importance in this problem's context.
In basic terms, the cost of communication is high or comparable to the cost of invocation, but whereas invocation is unavoidable, communication can be avoided by assigning all actors to one processor.
This is an artefact of the simplification of the model discussed in Chapter \ref{chapModel}.
A more complex, and useful, model would be accounting for the parallel makespan, rather than the total cost.
This would encourage a tradeoff which assigns some actors even to bad processors so as to ensure a shorter runtime.

In practice the only way to encourage processor utilisation with the simplified model is to suppose heterogeneous architectures in the target machine.
In the real world, certain kinds of processors are more suited to different tasks.
A GPGPU may be able to perform matrix multiplications quickly, but has much less speedup for sorting tasks, a CPU in contrast is good at sorting and less efficient at the highly parallelisable task of matrix multiplication.
To this end we can rely on highly divergent invocation times for the different processors.
This, coupled with increasing the cost of invocation in comparison to communication, should encourage utilisation of more processors.

\begin{table}
\begin{center}
	\input{figures/exScale}
\caption{Processor utilisation with increasing invocation cost}
\label{figExScale}
\end{center}
\end{table}

Table \ref{figExScale} shows the average utilisation of processors with increasing costs of communication.
These experiments are run to assign eight actors to five processors, where in half the tests the eight actors are all stand-alone, and in the other half they are the two-way replication of four stand-alone actors.
We fix the communication costs to a random range for all tests and increase the invocation costs for different tests.
Utilisation shows the average proportion of the processors that were used.
The columns of this table show:
\begin{itemize}
	\item {\bf Scale}: The ratio of actor invocation costs over communication costs.
	\item {\bf Range}: The actual range of invocation times for all actors in all experiments for the given scale.
	\item {\bf AO}: The average optimal assignment processor utilisation for all experiments in this scale.
	\item {\bf NDO}: As above for only the un-duplicated test cases.
	\item {\bf DO}: As above for only the duplicated test cases.
	\item {\bf AH}: The average heuristic assignment processor utilisation for all experiments in this scale.
	\item {\bf NDH}: As above for only the un-duplicated test cases.
	\item {\bf DH}: As above for only the duplicated test cases.
\end{itemize}

Examining this data we see what we expected.
The processor utilisation increases as we increase the cost of invocation compared with communication.
For these experiments the proportion of communication cost remains constant, so eventually the tradeoff to distribute processors becomes viable.
However, we see no pattern when comparing the heuristic to the optimal utilisations, and heuristic assignments will sporadically use more or less processors than the optimal.

