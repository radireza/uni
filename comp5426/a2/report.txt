COMP5426 - Assignment 2

Nic Hollingum

308193415

Nonblocking Send and Receive
	This requires a simple modification on the original blocking send and receive problem with a few modifications.  First we must make the master a passive entity awaiting requests from the workers.  The master fills a worker request as best it can or terminates that worker.  Termination is achieved by assigning a work buffer of 0 length, which indicates that there is no more work to do, in this case communication is saved by not sending the 0 length buffer and the worker terminates immediately.
	Only nonblocking receive is used on the master's side to improve efficiency, all of the worker's calls are blocking, as it is not possible for the worker to do anything while waiting between calls.  The master on the otherhand can service other workers while waiting for one particular worker, so this requires the use of nonblocking receive. In the case that there are no workers the master simply does everything itself, for the rest of the discussion we will presume there is at least 1 worker.
	The master spawns a pending request for each worker and calls 'MPI_Waitsome' until at least 1 worker has requested.  Each worker will initialy request by sending '0' to the server, for simplicity of code this number will be the running sum for that workload, and so since no workload has been given that sum is 0 for now.  The master then reads a blocking request for that worker's desired work ammount, a random positive number between 2 and the maximum block size.  The master will then send the actual work ammount (which is usually equal to the requested ammount, unless the master runs out of input) followed by the data.  If the master did not reach the end of input then it will setup a new nonblocking receive for that worker to wait for the worker to finish.  The master continues to service workers in this fashion.  When the master does run out of input, when a worker finishes and makes a new request, the master terminates it and a new nonblocking receive is not setup.
	By this mechanism, the master is able to service workers out of order, and possibly out of sequence.  In the previous code the master synchronised all worker completions in order and dispatched all tasks in order.  With nonblocking receive the master services workers as they finish, and is able to service the same worker possibly multiple times between the time it takes a different worker to finish one workload, as indeed may happen due to the variable size of workloads.

Redblue Simulation
	The challenge in this task is to acheive deterministic output in a threaded execution.  Due to the lack of bounds on the input and output formats we will make some presumptions.  The size of the grid and the size of the tiles is specified at compile time by modifying T and TW in the source code, the number of tiles per side, and the width (in cells per tile) of each tile.  There will be TxT tiles total in the grid, each with TWxTW cells, so N is TxTW, and there are TxTWxTxTW cells total.
	The grid is initialised with random data.  A method to initialise the grid with data deterministically has been provided, however in the current implementation the random initialisation is used.  Note this causes nondeterministic output because of the randomness in the grid, NOT because of nondeterminism in the threaded behaviour.
	For the current implementation the grid has 4 tiles (T=2) and each tile has 9 cells (TW=3), there are 36 cells in the grid, 12 of each colour.  This grid is seeded randomly at execution, then the flow simulation begins.
	At each iteration, the current state of the grid is printed to screen, and some threads determine if any tile has more than PERCENT percent of either colour (not white).  PERCENT is a constant defined in the source code, currently it is 66, which equates to the requirement for 6 of a given tile colour in a single cell.  There is one such 'check thread' for each tile, all check threads run independantly and non-conflictingly with each other.  There are synchronisation mechanisms which will be described later to ensure all check threads start at a certain time, and finish at a certain time.
	If no tile has more than the required number of coloured cells, then the movement stage begins.  Here a thread for each column moves all the red blocks that can be moved to the right.  When all rows are finished, the same threads swap to moving the blue's in each column.  The synchronisation method used to keep att these threads in check is described below.  Suffice it to say that no blue thread may move pieces while any red thread is still executin, and vice versa.
	All threads are synchronised using 2 phase loop locking.  Each thread has 2 mutices and 1 condition, and operates in a loop.  The loop is terminated by global flags 'stopcall' and 'going', which have their own mutex to prevent write conflicts.  Each thread blocks at the 'initial' synchronisation point, for check threads this is the 'check' lock, and for movement threads this is the 'blue' lock.  The mthod 'lockstep' is called to block the main thread while all of a given kind of thread arrive at the sync point.  When all threads arrive the signal is given and the thread advances to the next stage in its loop.  For the check threads this is the 'done' lock and for the movement threads this is the 'red' lock.  Calling lockstep for the other set of locks will allow the thread to execute more, taking the loop and eventually ariving back at the initial synchronisation point.
	2 phase loop locking allows us to execute data parallel segments independantly without conflicts, but ensures synchrony when required.  For example the same thread used to move th reds on row 3, is used to move the blues in column 3.  But that thread is not allowed to move blues until all movement threads have finished moving reds.  Similarly no checking thread is allowed to check the grid until all movement threads have finished moving blues, and no movement thread is allowed to begin moving reds until all checking threads are finished checking their tile.
	This locking protocol allows the maximum possible parallelism whilst ensuring deterministic results (for the same grid, the grids are randomly generated so the behaviour appears nondeterministic).
	This version uses c=66%, N=6 and T=t because this provides some interesting results.  Most often the flow will reach MAX_ITERATIONS (100) without finding saturation, however occasionally it will terminate earlier.  Early termination is due sometimes to the initial seed creating a saturated tile, and so 0 iterations are required.  However sometimes 1-5 iterations will be required.  Setting C to be higher dramatically increases the number of simulations that run to MAX_ITERATIONS(indeed, setting C > 100 will cause this always).  And setting it too low will cause too many instances to terminate instantly (the random generation will often make a tile with 50% of one colour).